{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99815102",
   "metadata": {},
   "source": [
    "# ðŸ““ Notebook 04: Model Evaluation\n",
    "## Há»‡ thá»‘ng Recommendation Phim\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "1. Load trained models\n",
    "2. Load test data\n",
    "3. TÃ­nh metrics cho má»—i model (RMSE, MAE, Precision@K, Recall@K, etc.)\n",
    "4. So sÃ¡nh hiá»‡u suáº¥t cÃ¡c models\n",
    "5. LÆ°u metrics vÃ o MongoDB\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55b73a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e7d6c",
   "metadata": {},
   "source": [
    "## 2. Configuration & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b367e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "DATABASE_NAME = \"movie_recommendation\"\n",
    "MODELS_DIR = \"../models_saved\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Evaluation parameters\n",
    "TOP_K_VALUES = [5, 10, 20]  # For Precision@K, Recall@K, etc.\n",
    "RATING_THRESHOLD = 3.5  # Threshold for considering a movie as \"liked\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DATABASE_NAME]\n",
    "\n",
    "print(f\"Connected to MongoDB: {DATABASE_NAME}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd8ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movies and ratings data\n",
    "movies_df = pd.read_csv(f\"{PROCESSED_DATA_DIR}/movies_processed.csv\")\n",
    "ratings_df = pd.read_csv(f\"{PROCESSED_DATA_DIR}/ratings_processed.csv\")\n",
    "\n",
    "# Load test data (if available, otherwise create train/test split)\n",
    "try:\n",
    "    test_ratings = pd.read_csv(f\"{PROCESSED_DATA_DIR}/test_ratings.csv\")\n",
    "    train_ratings = pd.read_csv(f\"{PROCESSED_DATA_DIR}/train_ratings.csv\")\n",
    "    print(\"Loaded existing train/test split\")\n",
    "except FileNotFoundError:\n",
    "    # Create train/test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_ratings, test_ratings = train_test_split(\n",
    "        ratings_df, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_ratings.to_csv(f\"{PROCESSED_DATA_DIR}/train_ratings.csv\", index=False)\n",
    "    test_ratings.to_csv(f\"{PROCESSED_DATA_DIR}/test_ratings.csv\", index=False)\n",
    "    print(\"Created new train/test split\")\n",
    "\n",
    "print(f\"\\nMovies: {len(movies_df)}\")\n",
    "print(f\"Total ratings: {len(ratings_df)}\")\n",
    "print(f\"Train ratings: {len(train_ratings)}\")\n",
    "print(f\"Test ratings: {len(test_ratings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e606fd6",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all trained models\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load a trained model from pickle file\"\"\"\n",
    "    model_path = f\"{MODELS_DIR}/{model_name}.pkl\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"âœ“ Loaded {model_name}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âœ— Model not found: {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Load all models\n",
    "models = {\n",
    "    'content_based': load_model('content_based_model'),\n",
    "    'item_based': load_model('item_based_model'),\n",
    "    'user_based': load_model('user_based_model'),\n",
    "    'hybrid': load_model('hybrid_model')\n",
    "}\n",
    "\n",
    "# Check which models are available\n",
    "available_models = {k: v for k, v in models.items() if v is not None}\n",
    "print(f\"\\nAvailable models: {list(available_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7755df7",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Metrics\n",
    "\n",
    "### 4.1 Prediction Accuracy Metrics (RMSE, MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Root Mean Squared Error\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Error\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Test the metrics\n",
    "y_true_test = [4.0, 3.5, 5.0, 2.0, 4.5]\n",
    "y_pred_test = [3.8, 3.7, 4.8, 2.3, 4.2]\n",
    "print(f\"Test RMSE: {rmse(y_true_test, y_pred_test):.4f}\")\n",
    "print(f\"Test MAE: {mae(y_true_test, y_pred_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302a299",
   "metadata": {},
   "source": [
    "### 4.2 Ranking Metrics (Precision@K, Recall@K, F1@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended, relevant, k):\n",
    "    \"\"\"\n",
    "    Precision@K: Proportion of recommended items that are relevant\n",
    "    \n",
    "    Args:\n",
    "        recommended: List of recommended item IDs (ordered by rank)\n",
    "        relevant: Set of relevant item IDs (items user actually liked)\n",
    "        k: Number of top recommendations to consider\n",
    "    \"\"\"\n",
    "    recommended_k = recommended[:k]\n",
    "    hits = len(set(recommended_k) & set(relevant))\n",
    "    return hits / k if k > 0 else 0.0\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    \"\"\"\n",
    "    Recall@K: Proportion of relevant items that are recommended\n",
    "    \n",
    "    Args:\n",
    "        recommended: List of recommended item IDs (ordered by rank)\n",
    "        relevant: Set of relevant item IDs (items user actually liked)\n",
    "        k: Number of top recommendations to consider\n",
    "    \"\"\"\n",
    "    if len(relevant) == 0:\n",
    "        return 0.0\n",
    "    recommended_k = recommended[:k]\n",
    "    hits = len(set(recommended_k) & set(relevant))\n",
    "    return hits / len(relevant)\n",
    "\n",
    "def f1_at_k(recommended, relevant, k):\n",
    "    \"\"\"\n",
    "    F1@K: Harmonic mean of Precision@K and Recall@K\n",
    "    \"\"\"\n",
    "    p = precision_at_k(recommended, relevant, k)\n",
    "    r = recall_at_k(recommended, relevant, k)\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "# Test the ranking metrics\n",
    "recommended_test = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "relevant_test = {2, 5, 8, 11, 15}\n",
    "\n",
    "for k in [5, 10]:\n",
    "    print(f\"K={k}:\")\n",
    "    print(f\"  Precision@{k}: {precision_at_k(recommended_test, relevant_test, k):.4f}\")\n",
    "    print(f\"  Recall@{k}: {recall_at_k(recommended_test, relevant_test, k):.4f}\")\n",
    "    print(f\"  F1@{k}: {f1_at_k(recommended_test, relevant_test, k):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761b2f0",
   "metadata": {},
   "source": [
    "### 4.3 NDCG@K (Normalized Discounted Cumulative Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevance_scores, k):\n",
    "    \"\"\"\n",
    "    Discounted Cumulative Gain at K\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores in ranked order\n",
    "        k: Number of items to consider\n",
    "    \"\"\"\n",
    "    relevance_scores = np.array(relevance_scores[:k])\n",
    "    if len(relevance_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # DCG = sum(rel_i / log2(i+2)) for i from 0 to k-1\n",
    "    discounts = np.log2(np.arange(2, len(relevance_scores) + 2))\n",
    "    return np.sum(relevance_scores / discounts)\n",
    "\n",
    "def ndcg_at_k(recommended, relevant_with_scores, k):\n",
    "    \"\"\"\n",
    "    Normalized Discounted Cumulative Gain at K\n",
    "    \n",
    "    Args:\n",
    "        recommended: List of recommended item IDs (ordered by rank)\n",
    "        relevant_with_scores: Dict mapping item_id -> relevance score (e.g., rating)\n",
    "        k: Number of items to consider\n",
    "    \"\"\"\n",
    "    # Get relevance scores for recommended items\n",
    "    recommended_k = recommended[:k]\n",
    "    relevance_scores = [relevant_with_scores.get(item, 0) for item in recommended_k]\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = dcg_at_k(relevance_scores, k)\n",
    "    \n",
    "    # Calculate ideal DCG (sorted relevance scores)\n",
    "    ideal_scores = sorted(relevant_with_scores.values(), reverse=True)[:k]\n",
    "    idcg = dcg_at_k(ideal_scores, k)\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "# Test NDCG\n",
    "recommended_test = [1, 2, 3, 4, 5]\n",
    "relevant_with_scores_test = {1: 5, 2: 3, 3: 1, 6: 4, 7: 2}\n",
    "\n",
    "for k in [3, 5]:\n",
    "    print(f\"NDCG@{k}: {ndcg_at_k(recommended_test, relevant_with_scores_test, k):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad24daa",
   "metadata": {},
   "source": [
    "### 4.4 Coverage (Catalog Coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bfffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_coverage(all_recommendations, total_items):\n",
    "    \"\"\"\n",
    "    Catalog Coverage: Proportion of items that appear in any recommendation list\n",
    "    \n",
    "    Args:\n",
    "        all_recommendations: List of all recommendation lists for all users\n",
    "        total_items: Total number of items in catalog\n",
    "    \"\"\"\n",
    "    unique_recommended = set()\n",
    "    for recs in all_recommendations:\n",
    "        unique_recommended.update(recs)\n",
    "    \n",
    "    return len(unique_recommended) / total_items if total_items > 0 else 0.0\n",
    "\n",
    "def diversity(recommendations, item_similarity_matrix=None):\n",
    "    \"\"\"\n",
    "    Intra-list Diversity: Average dissimilarity between recommended items\n",
    "    \n",
    "    Args:\n",
    "        recommendations: List of recommended item IDs\n",
    "        item_similarity_matrix: Pre-computed similarity matrix (optional)\n",
    "    \"\"\"\n",
    "    if len(recommendations) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # If no similarity matrix, return 1 (maximum diversity assumed)\n",
    "    if item_similarity_matrix is None:\n",
    "        return 1.0\n",
    "    \n",
    "    total_dissimilarity = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(recommendations)):\n",
    "        for j in range(i + 1, len(recommendations)):\n",
    "            item_i, item_j = recommendations[i], recommendations[j]\n",
    "            if item_i in item_similarity_matrix.index and item_j in item_similarity_matrix.columns:\n",
    "                similarity = item_similarity_matrix.loc[item_i, item_j]\n",
    "                total_dissimilarity += (1 - similarity)\n",
    "            else:\n",
    "                total_dissimilarity += 1  # Assume dissimilar if not in matrix\n",
    "            count += 1\n",
    "    \n",
    "    return total_dissimilarity / count if count > 0 else 0.0\n",
    "\n",
    "# Test coverage\n",
    "all_recs_test = [[1, 2, 3], [2, 3, 4], [1, 5, 6]]\n",
    "print(f\"Catalog Coverage: {catalog_coverage(all_recs_test, 10):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4eb511",
   "metadata": {},
   "source": [
    "## 5. Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_relevant_items(user_id, ratings_df, threshold=RATING_THRESHOLD):\n",
    "    \"\"\"Get items that user rated >= threshold (considered 'liked')\"\"\"\n",
    "    user_ratings = ratings_df[ratings_df['userId'] == user_id]\n",
    "    relevant = user_ratings[user_ratings['rating'] >= threshold]['movieId'].tolist()\n",
    "    return set(relevant)\n",
    "\n",
    "def get_user_ratings_dict(user_id, ratings_df):\n",
    "    \"\"\"Get dictionary of movie_id -> rating for a user\"\"\"\n",
    "    user_ratings = ratings_df[ratings_df['userId'] == user_id]\n",
    "    return dict(zip(user_ratings['movieId'], user_ratings['rating']))\n",
    "\n",
    "def evaluate_model_for_user(model, model_name, user_id, train_ratings, test_ratings, \n",
    "                            movies_df, k_values=TOP_K_VALUES):\n",
    "    \"\"\"\n",
    "    Evaluate a model for a single user\n",
    "    \n",
    "    Returns dict with metrics for each k value\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Get relevant items from test set\n",
    "    relevant_items = get_user_relevant_items(user_id, test_ratings)\n",
    "    relevant_with_scores = get_user_ratings_dict(user_id, test_ratings)\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return None  # Skip users with no relevant items in test set\n",
    "    \n",
    "    # Get recommendations\n",
    "    try:\n",
    "        if hasattr(model, 'recommend_for_user'):\n",
    "            recommendations = model.recommend_for_user(user_id, n=max(k_values))\n",
    "        elif hasattr(model, 'recommend'):\n",
    "            recommendations = model.recommend(user_id, n=max(k_values))\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Extract movie IDs from recommendations\n",
    "        if isinstance(recommendations, list) and len(recommendations) > 0:\n",
    "            if isinstance(recommendations[0], dict):\n",
    "                rec_ids = [r.get('movieId', r.get('movie_id')) for r in recommendations]\n",
    "            elif isinstance(recommendations[0], tuple):\n",
    "                rec_ids = [r[0] for r in recommendations]\n",
    "            else:\n",
    "                rec_ids = recommendations\n",
    "        else:\n",
    "            rec_ids = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting recommendations for user {user_id}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics for each k\n",
    "    for k in k_values:\n",
    "        results[f'precision@{k}'] = precision_at_k(rec_ids, relevant_items, k)\n",
    "        results[f'recall@{k}'] = recall_at_k(rec_ids, relevant_items, k)\n",
    "        results[f'f1@{k}'] = f1_at_k(rec_ids, relevant_items, k)\n",
    "        results[f'ndcg@{k}'] = ndcg_at_k(rec_ids, relevant_with_scores, k)\n",
    "    \n",
    "    results['recommendations'] = rec_ids\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b58f9f",
   "metadata": {},
   "source": [
    "## 6. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1540092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, train_ratings, test_ratings, movies_df, \n",
    "                   sample_users=100, k_values=TOP_K_VALUES):\n",
    "    \"\"\"\n",
    "    Evaluate a model across multiple users\n",
    "    \n",
    "    Args:\n",
    "        model: The recommendation model\n",
    "        model_name: Name of the model\n",
    "        train_ratings: Training ratings DataFrame\n",
    "        test_ratings: Test ratings DataFrame\n",
    "        movies_df: Movies DataFrame\n",
    "        sample_users: Number of users to sample for evaluation\n",
    "        k_values: List of K values for ranking metrics\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with aggregated metrics\n",
    "    \"\"\"\n",
    "    # Get users who have ratings in both train and test sets\n",
    "    train_users = set(train_ratings['userId'].unique())\n",
    "    test_users = set(test_ratings['userId'].unique())\n",
    "    common_users = list(train_users & test_users)\n",
    "    \n",
    "    # Sample users\n",
    "    if len(common_users) > sample_users:\n",
    "        np.random.seed(42)\n",
    "        sample_user_ids = np.random.choice(common_users, sample_users, replace=False)\n",
    "    else:\n",
    "        sample_user_ids = common_users\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name} on {len(sample_user_ids)} users...\")\n",
    "    \n",
    "    # Collect results\n",
    "    all_results = []\n",
    "    all_recommendations = []\n",
    "    \n",
    "    for user_id in tqdm(sample_user_ids, desc=f\"Evaluating {model_name}\"):\n",
    "        result = evaluate_model_for_user(\n",
    "            model, model_name, user_id, train_ratings, test_ratings, movies_df, k_values\n",
    "        )\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "            all_recommendations.append(result['recommendations'])\n",
    "    \n",
    "    if len(all_results) == 0:\n",
    "        print(f\"No valid results for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Aggregate results\n",
    "    aggregated = {'model': model_name, 'num_users': len(all_results)}\n",
    "    \n",
    "    # Average ranking metrics\n",
    "    for k in k_values:\n",
    "        for metric in ['precision', 'recall', 'f1', 'ndcg']:\n",
    "            key = f'{metric}@{k}'\n",
    "            values = [r[key] for r in all_results]\n",
    "            aggregated[key] = np.mean(values)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    total_items = len(movies_df['movieId'].unique())\n",
    "    aggregated['coverage'] = catalog_coverage(all_recommendations, total_items)\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "print(\"Model evaluation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all available models\n",
    "evaluation_results = []\n",
    "\n",
    "for model_name, model in available_models.items():\n",
    "    result = evaluate_model(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        train_ratings=train_ratings,\n",
    "        test_ratings=test_ratings,\n",
    "        movies_df=movies_df,\n",
    "        sample_users=100,\n",
    "        k_values=TOP_K_VALUES\n",
    "    )\n",
    "    if result:\n",
    "        evaluation_results.append(result)\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"  Coverage: {result['coverage']:.4f}\")\n",
    "        for k in TOP_K_VALUES:\n",
    "            print(f\"  Precision@{k}: {result[f'precision@{k}']:.4f}\")\n",
    "            print(f\"  Recall@{k}: {result[f'recall@{k}']:.4f}\")\n",
    "            print(f\"  NDCG@{k}: {result[f'ndcg@{k}']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Evaluation completed for {len(evaluation_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ee0f4",
   "metadata": {},
   "source": [
    "## 7. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "if evaluation_results:\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    # Reorder columns for better display\n",
    "    col_order = ['model', 'num_users', 'coverage']\n",
    "    for k in TOP_K_VALUES:\n",
    "        col_order.extend([f'precision@{k}', f'recall@{k}', f'f1@{k}', f'ndcg@{k}'])\n",
    "    \n",
    "    # Keep only columns that exist\n",
    "    col_order = [c for c in col_order if c in results_df.columns]\n",
    "    results_df = results_df[col_order]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL COMPARISON RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    display(results_df.round(4))\n",
    "else:\n",
    "    print(\"No evaluation results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b0a85",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72daac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_results:\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    models_list = [r['model'] for r in evaluation_results]\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(models_list)))\n",
    "    \n",
    "    # Plot 1: Precision@K comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    x = np.arange(len(TOP_K_VALUES))\n",
    "    width = 0.2\n",
    "    for i, model_name in enumerate(models_list):\n",
    "        result = next(r for r in evaluation_results if r['model'] == model_name)\n",
    "        precision_values = [result[f'precision@{k}'] for k in TOP_K_VALUES]\n",
    "        ax1.bar(x + i * width, precision_values, width, label=model_name, color=colors[i])\n",
    "    ax1.set_xlabel('K')\n",
    "    ax1.set_ylabel('Precision@K')\n",
    "    ax1.set_title('Precision@K Comparison')\n",
    "    ax1.set_xticks(x + width * (len(models_list) - 1) / 2)\n",
    "    ax1.set_xticklabels([f'K={k}' for k in TOP_K_VALUES])\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Recall@K comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    for i, model_name in enumerate(models_list):\n",
    "        result = next(r for r in evaluation_results if r['model'] == model_name)\n",
    "        recall_values = [result[f'recall@{k}'] for k in TOP_K_VALUES]\n",
    "        ax2.bar(x + i * width, recall_values, width, label=model_name, color=colors[i])\n",
    "    ax2.set_xlabel('K')\n",
    "    ax2.set_ylabel('Recall@K')\n",
    "    ax2.set_title('Recall@K Comparison')\n",
    "    ax2.set_xticks(x + width * (len(models_list) - 1) / 2)\n",
    "    ax2.set_xticklabels([f'K={k}' for k in TOP_K_VALUES])\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Plot 3: NDCG@K comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    for i, model_name in enumerate(models_list):\n",
    "        result = next(r for r in evaluation_results if r['model'] == model_name)\n",
    "        ndcg_values = [result[f'ndcg@{k}'] for k in TOP_K_VALUES]\n",
    "        ax3.bar(x + i * width, ndcg_values, width, label=model_name, color=colors[i])\n",
    "    ax3.set_xlabel('K')\n",
    "    ax3.set_ylabel('NDCG@K')\n",
    "    ax3.set_title('NDCG@K Comparison')\n",
    "    ax3.set_xticks(x + width * (len(models_list) - 1) / 2)\n",
    "    ax3.set_xticklabels([f'K={k}' for k in TOP_K_VALUES])\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Plot 4: Coverage comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    coverage_values = [r['coverage'] for r in evaluation_results]\n",
    "    bars = ax4.bar(models_list, coverage_values, color=colors[:len(models_list)])\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Coverage')\n",
    "    ax4.set_title('Catalog Coverage Comparison')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, coverage_values):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{val:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nChart saved to: ../data/processed/model_comparison.png\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1068e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for overall comparison\n",
    "if evaluation_results and len(evaluation_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Metrics for radar chart (using K=10)\n",
    "    k = 10\n",
    "    metrics = [f'precision@{k}', f'recall@{k}', f'f1@{k}', f'ndcg@{k}', 'coverage']\n",
    "    metric_labels = [f'Precision@{k}', f'Recall@{k}', f'F1@{k}', f'NDCG@{k}', 'Coverage']\n",
    "    \n",
    "    # Number of metrics\n",
    "    num_metrics = len(metrics)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    for i, result in enumerate(evaluation_results):\n",
    "        values = [result.get(m, 0) for m in metrics]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=result['model'], color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_labels)\n",
    "    ax.set_title(f'Model Comparison Radar Chart (K={k})', size=14, y=1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/model_radar_chart.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nRadar chart saved to: ../data/processed/model_radar_chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488392",
   "metadata": {},
   "source": [
    "## 9. Save Metrics to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results to MongoDB\n",
    "if evaluation_results:\n",
    "    # Create metrics collection\n",
    "    metrics_collection = db['model_metrics']\n",
    "    \n",
    "    # Prepare documents for insertion\n",
    "    evaluation_documents = []\n",
    "    \n",
    "    for result in evaluation_results:\n",
    "        doc = {\n",
    "            'model_name': result['model'],\n",
    "            'evaluation_date': datetime.now(),\n",
    "            'num_users_evaluated': result['num_users'],\n",
    "            'rating_threshold': RATING_THRESHOLD,\n",
    "            'metrics': {},\n",
    "            'parameters': {\n",
    "                'k_values': TOP_K_VALUES,\n",
    "                'sample_users': result['num_users']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add metrics\n",
    "        for k in TOP_K_VALUES:\n",
    "            doc['metrics'][f'precision_at_{k}'] = result[f'precision@{k}']\n",
    "            doc['metrics'][f'recall_at_{k}'] = result[f'recall@{k}']\n",
    "            doc['metrics'][f'f1_at_{k}'] = result[f'f1@{k}']\n",
    "            doc['metrics'][f'ndcg_at_{k}'] = result[f'ndcg@{k}']\n",
    "        \n",
    "        doc['metrics']['coverage'] = result['coverage']\n",
    "        \n",
    "        evaluation_documents.append(doc)\n",
    "    \n",
    "    # Insert into MongoDB\n",
    "    result = metrics_collection.insert_many(evaluation_documents)\n",
    "    \n",
    "    print(f\"âœ“ Saved {len(result.inserted_ids)} evaluation results to MongoDB\")\n",
    "    print(f\"  Collection: model_metrics\")\n",
    "    print(f\"  Inserted IDs: {result.inserted_ids}\")\n",
    "else:\n",
    "    print(\"No results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved data\n",
    "print(\"\\n=== Verification: Data in MongoDB ===\")\n",
    "saved_metrics = list(metrics_collection.find().sort('evaluation_date', -1).limit(4))\n",
    "\n",
    "for metric in saved_metrics:\n",
    "    print(f\"\\nModel: {metric['model_name']}\")\n",
    "    print(f\"  Evaluated on: {metric['evaluation_date']}\")\n",
    "    print(f\"  Users evaluated: {metric['num_users_evaluated']}\")\n",
    "    print(f\"  Coverage: {metric['metrics']['coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3dad7",
   "metadata": {},
   "source": [
    "## 10. Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for reference\n",
    "if evaluation_results:\n",
    "    results_df['evaluation_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    csv_path = '../data/processed/model_evaluation_results.csv'\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ“ Results saved to: {csv_path}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff170ed7",
   "metadata": {},
   "source": [
    "## 11. Summary & Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "- **Best Precision@K**: Model with highest precision correctly identifies relevant items\n",
    "- **Best Recall@K**: Model with highest recall captures more relevant items\n",
    "- **Best NDCG@K**: Model with highest NDCG provides better ranking quality\n",
    "- **Best Coverage**: Model that recommends the most diverse set of items\n",
    "\n",
    "### Recommendations:\n",
    "1. **Hybrid model** typically performs best as it combines strengths of multiple approaches\n",
    "2. **Content-based** is useful for cold-start (new users/items)\n",
    "3. **Collaborative filtering** excels when sufficient rating data exists\n",
    "4. Consider using **ensemble methods** to further improve performance\n",
    "\n",
    "### Next Steps:\n",
    "1. Fine-tune model hyperparameters based on evaluation results\n",
    "2. Implement A/B testing for production deployment\n",
    "3. Add more evaluation metrics (novelty, serendipity)\n",
    "4. Monitor model performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a12cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close MongoDB connection\n",
    "client.close()\n",
    "print(\"âœ“ MongoDB connection closed\")\n",
    "print(\"\\nðŸŽ‰ Model evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
